{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66a172de-0cd7-48eb-8164-771c9adaccf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from typing import List,Tuple\n",
    "from copy import deepcopy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df2a219-c3c3-4d11-bf56-930bfecabf7b",
   "metadata": {},
   "source": [
    "# operating base for our nerual network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "27ab5ea0-fc92-4029-8791-331428cb05d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_same_shape(array:np.ndarray,\n",
    "                     array_grad:np.ndarray)->None:\n",
    "    assert array.shape == array_grad.shape,\\\n",
    "        '''the shape of the nd array should be the same \n",
    "          but the shape of the array's are {0} for the first one and\n",
    "          the second array shape are {1}'''.format(tuple(array.shape),tuple(array_grad.shape))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "8505375b-9350-4fdb-b2dc-bd0a7ca76c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class operation(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self,input_:np.ndarray):\n",
    "        self.input_=input_\n",
    "        self.output=self._output()\n",
    "        return self.output\n",
    "    def backward(self,output_grad:np.ndarray)->np.ndarray:\n",
    "        assert_same_shape(self.output,output_grad)\n",
    "        self.input_grad=self._input_grad(output_grad)\n",
    "        assert_same_shape(self.input_,self.input_grad)\n",
    "        return self.input_grad\n",
    "    def _output(self)->np.ndarray:\n",
    "        raise NotImplementedError()\n",
    "    def _input_grad(self,output_grad:np.ndarray)->np.ndarray:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "3066212b-9156-466f-b308-ef0b0473c678",
   "metadata": {},
   "outputs": [],
   "source": [
    "class paramoperation(operation):\n",
    "    def __init__(self,param:np.ndarray):\n",
    "        super().__init__()\n",
    "        self.param=param\n",
    "        \n",
    "    def backward(self,output_grad:np.ndarray):\n",
    "        assert_same_shape(self.output,output_grad)\n",
    "        self.input_grad=self._input_grad(output_grad)\n",
    "        self.param_grad=self._param_grad(output_grad)\n",
    "        #we have to check for the shapes of the variables\n",
    "        assert_same_shape(self.input_,self.input_grad)\n",
    "        assert_same_shape(self.param,self.param_grad)\n",
    "        return self.input_grad\n",
    "        \n",
    "    def _param_grad(output_grad:np.ndarray):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "38ad5d56-c438-49fa-9927-263d03baa012",
   "metadata": {},
   "outputs": [],
   "source": [
    "class weightmultiply(paramoperation):\n",
    "    def __init__(self,w:np.ndarray):\n",
    "        super().__init__(w)  #this is going to be send to the parent class\n",
    "    def _output(self)->np.ndarray:\n",
    "        return np.dot(self.input_,self.param)  #this is the first forward of the nerual network\n",
    "        \n",
    "    def _input_grad(self,output_grad:np.ndarray)->np.ndarray:\n",
    "        ''' we are dervating with respect with the input'''\n",
    "        return np.dot(output_grad,np.transpose(self.param,(1,0)))\n",
    "         \n",
    "    def _param_grad(self,output_grad:np.ndarray)->np.ndarray:\n",
    "        '''we are derivating with respect to the parameters or the weights '''\n",
    "        return np.dot(np.transpose(self.input_,(1,0)),output_grad)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "7c034ca7-139b-4125-b71b-6c43da342f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class biasadd(paramoperation):\n",
    "    \n",
    "    def __init__(self,bias):\n",
    "        assert bias.shape[0]==1\n",
    "        super().__init__(bias)\n",
    "        \n",
    "    def _output(self):\n",
    "        ''' the other values is being treated as bias'''\n",
    "        return self.input_ + self.param\n",
    "        \n",
    "    def _input_grad(self,output_grad:np.ndarray)->np.ndarray:\n",
    "        ''' this is with respect with the bias '''\n",
    "        return np.ones_like(self.input_)*output_grad\n",
    "        \n",
    "    def _param_grad(self,output_grad:np.ndarray)->np.ndarray:\n",
    "        '''this derivation is with respect to the bias'''\n",
    "        param_grad= np.ones_like(self.param)*output_grad\n",
    "        return np.sum(param_grad,axis=0).reshape(1,param_grad.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "095ab7f7-5813-40d3-b5d5-63c5e5e90c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sigmoid(operation):\n",
    "    '''this is sigmoid function'''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def _output(self):\n",
    "        return 1.0/(1.0+np.exp(-1.0*self.input_))\n",
    "        \n",
    "    def _input_grad(self,output_grad:np.ndarray):\n",
    "        ''' this is the input grad with respect to the sigmoid fucntion'''\n",
    "        sigmoid_back=self.output*(1-self.output)\n",
    "        input_grad=sigmoid_back*output_grad\n",
    "        return input_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "355d7d49-49f8-4008-81cf-f43080d9d1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear(operation):\n",
    "    ''' this is identity activation function for our A.N.N'''\n",
    "    def __init__(self)->None:\n",
    "        super().__init__()\n",
    "    def _output(self)->np.ndarray:\n",
    "        return self.input_\n",
    "    def _input_grad(self,output_grad:np.ndarray)->np.ndarray:\n",
    "        return output_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3324743-9583-4c8c-9abe-aae0f259267b",
   "metadata": {},
   "source": [
    "# this is layers of neural network from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "91792870-2269-4e75-90c2-ffb570a39643",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    \n",
    "    def __init__(self,neurons:int):\n",
    "        \n",
    "        self.neurons=neurons\n",
    "        self.first=True\n",
    "        \n",
    "        self.param:List[np.ndarray]=[]\n",
    "        self.param_grads:List[np.ndarray]=[]\n",
    "        \n",
    "        self.operations:List[operation]=[]\n",
    "        \n",
    "    def _setup_layer(self,num_in:int):\n",
    "        raise NotImplementedEror()\n",
    "        \n",
    "    def forward(self,input_:np.ndarray):\n",
    "        if self.first:\n",
    "            self._setup_layer(input_)\n",
    "            self.first=False\n",
    "        self.input_=input_\n",
    "        for operation in self.operations:\n",
    "            input_=operation.forward(input_)\n",
    "        self.output=input_\n",
    "        return self.output\n",
    "        \n",
    "    def backward(self,output_grad:np.ndarray)->np.ndarray:\n",
    "        '''lets check the appropriate shape of the output and \n",
    "        output grad and perform series of backpropagation'''\n",
    "        \n",
    "        assert_same_shape(self.output,output_grad)\n",
    "        \n",
    "        for operation in reversed(self.operations):\n",
    "            output_grad=operation.backward(output_grad)   # the operation is going back ward from output into input\n",
    "        \n",
    "        input_grad=output_grad  #because its moving from the far-left of the network to the right\n",
    "        self._params_grad()\n",
    "        \n",
    "        return input_grad\n",
    "    def _params_grad(self)->np.ndarray:\n",
    "        ''' this is usef to exrtact the param_grad from layers'''\n",
    "        self.param_grads=[]\n",
    "        for operation in self.operations:\n",
    "            if issubclass(operation.__class__,paramoperation):   #check if the class is a sub class of the \"paramoperation\" class\n",
    "                self.params.append(operation.param_grad)\n",
    "                \n",
    "    def _params(self)->np.ndarray:\n",
    "        ''' this is used to extract the param from the layers'''\n",
    "        self.param=[]\n",
    "        for operation in self.operations:\n",
    "            if subclass(operation.__class__,paramoperation):\n",
    "                self.param.appened(operation.param)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d67943b-6cf7-41ec-9ed2-e99e8df5427d",
   "metadata": {},
   "source": [
    "# know lets make dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "d433ddac-3340-4ca5-b9a5-25ff43e78686",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class dense(Layer):\n",
    "    ''' this is a fully conected layer inherted from layer'''\n",
    "    \n",
    "    def __init__(self,\n",
    "                 neurons:int,\n",
    "                 activation:operation=sigmoid()\n",
    "                )->None:\n",
    "        super().__init__(neurons)\n",
    "        self.activation=activation\n",
    "        \n",
    "    def _setup_layer(self,input_:np.ndarray):\n",
    "        \n",
    "        ''' this whole thing defines the function of a connected layer '''\n",
    "        \n",
    "        if self.seed:\n",
    "            np.random.seed(self.seed)\n",
    "        \n",
    "        self.params=[]\n",
    "        \n",
    "        #weights\n",
    "        self.params.append(np.random.randn(input_.shape[1],self.neurons))\n",
    "\n",
    "        #bias\n",
    "        self.params.append(np.random.randn(1,self.neurons))\n",
    "        \n",
    "        #the operation\n",
    "        self.operations=[weightmultiply(self.params[0]),\n",
    "                        biasadd(self.params[1]),\n",
    "                        self.activation]\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88feae9-04b5-40dd-a9f6-0cd393b74639",
   "metadata": {},
   "source": [
    "# lets make a base loss layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "4941d298-c59a-46e6-a369-8e2233a3b6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "    ''' the loss of a neural network '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self,prediction:np.ndarray,target:np.ndarray):\n",
    "        ''' this computes the loss of the neural net'''\n",
    "        assert_same_shape(prediction,target)\n",
    "        self.prediction=prediction\n",
    "        self.target=target\n",
    "        loss_value=self._output()\n",
    "        return loss_value\n",
    "    \n",
    "    def backward(self)->np.ndarray:\n",
    "        ''' this is for calculating the derivative with respect to the inputs'''\n",
    "        self.input_grad=self._input_grad()\n",
    "        assert_same_shape(self.prediction,self.input_grad)  #this is for checking the rule that said the input and output grad must be the same\n",
    "        return self.input_grad\n",
    "   \n",
    "    def _output(self)->float:\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def _input_grad(self):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "5e23b585-5ff0-4b60-8bfe-ea8e551cecf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class meansquarederror(Loss):\n",
    "    def __init__(self)->None:\n",
    "        super().__init__()\n",
    "    def _output(self)->float:\n",
    "        ''' this computes the mean squred error for the predicted and the truth'''\n",
    "        loss=(\n",
    "            np.sum(np.power(self.prediction - self.target,2))/self.prediction.shape[0]   # we are dividing  by the number of predictions to get mean squared error \n",
    "        )\n",
    "        return loss\n",
    "    def _input_grad(self)->np.ndarray:\n",
    "        ''' this calculated the gradient with respect to the input..in this case\n",
    "        the input is the predicted value\n",
    "        '''\n",
    "        return 2.0*(self.prediction - self.target)/self.prediction.shape[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a36ab9-fce7-4b48-bce5-6dce1a0d5d98",
   "metadata": {},
   "source": [
    "# lets build a fully connected neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "17b3068c-f41b-472d-a7cf-882c0ec20f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuralnetwork(object):\n",
    "    ''' we are going to make a neural network form all the operations,\n",
    "    layers and loss classes we have made so far '''\n",
    "    \n",
    "    def __init__(self,\n",
    "                layers:List[Layer],\n",
    "                loss:Loss,\n",
    "                seed: float=1):\n",
    "        \n",
    "        self.layers=layers\n",
    "        self.loss=loss\n",
    "        self.seed=seed\n",
    "        #lets make all the layers have the same seed number \n",
    "        if seed:\n",
    "            for layer in self.layers: \n",
    "                setattr(layer,\"seed\",self.seed)\n",
    "                \n",
    "    #this is the forward pass of the neural network\n",
    "    def forward(self,x_batch:np.ndarray):\n",
    "        x_out=x_batch\n",
    "        for layer in self.layers:\n",
    "            x_out=layer.forward(x_out)\n",
    "        return x_out\n",
    "            \n",
    "        #this is the backward pass of the nerual network\n",
    "    def backward(self,loss_grad:np.ndarray)->None:\n",
    "        grad=loss_grad  #this grad is the last layers grad\n",
    "        for layer in reversed(self.layers):\n",
    "            grad=layer.backward(grad)\n",
    "        return None\n",
    "        \n",
    "        #this computes the \"training\" part of the nerural network\n",
    "    def train_batch(self,x_batch,y_batch)->float:\n",
    "        prediction = self.forward(x_batch)\n",
    "        loss=self.loss.forward(prediction,y_batch)\n",
    "        self.backward(self.loss.backward())  #this is basically the backpropagation\n",
    "        return loss\n",
    "    def params(self):\n",
    "        ''' this is for retriving the parameters of the neural network'''\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            \n",
    "            yield from layer.params\n",
    "    def param_grads(self):\n",
    "        ''' this is for retreiving the parameter gradient'''\n",
    "        for layer in self.layers:\n",
    "             yield from layer.param_grads\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4bf0d5-fa12-4c5c-be90-6a6c8925fb12",
   "metadata": {},
   "source": [
    "# lets build optimizer and trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c07f85e-44a6-4d17-8e31-d68eb1117a15",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "4df9b340-8d8e-4de1-9a0d-18a53dd19f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class optimizer(object):\n",
    "    ''' this is for optimizing the weights of the neurla network '''\n",
    "    def __init__(self,lr:float=0.4):\n",
    "        '''we use learning rate specifying the rate at which the model is going to be build'''\n",
    "        self.lr=lr\n",
    "    def step(self):\n",
    "        '''this is for calculating or implmenting of the optimizing'''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "4e57fb3d-8883-432d-bff7-e0f463168171",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(optimizer):\n",
    "    ''' this optimizer is the stochastic gradient decent'''\n",
    "    def __init__(self,lr:float=0.01)->None:\n",
    "        super().__init__(lr)\n",
    "        \n",
    "    def step(self):\n",
    "        '''we adjust the parameters based on the learning rate of the model'''\n",
    "        for (param,param_grad) in zip(self.net.params(),self.net.param_grads()):\n",
    "            param-=self.lr*param_grad\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54694f94-6989-4d59-9ec6-39ce785b8ef7",
   "metadata": {},
   "source": [
    "## trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "96e06518-a85f-4f93-99cc-6eadd23cb690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_data(x:np.ndarray,y:np.ndarray):\n",
    "    perm=np.random.permutation(x.shape[0])\n",
    "    return x[perm],y[perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "41b40ff9-4931-4f54-ae92-4b60e86776de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    ''' this trains the neural network '''\n",
    "    def __init__(self,\n",
    "                net:neuralnetwork,\n",
    "                optim:optimizer)->None:\n",
    "        self.net=net\n",
    "        self.optim=optim\n",
    "        self.best_loss=1e9\n",
    "        setattr(self.optim,\"net\",self.net)  #the self.net is going to be an attribute of self.optim\n",
    "        \n",
    "    def generate_batches(self,\n",
    "                         x:np.ndarray,\n",
    "                         y:np.ndarray,\n",
    "                        size:int=32)->Tuple[np.ndarray]:\n",
    "        ''' this generates batch for the training'''\n",
    "        \n",
    "        assert x.shape[0]==y.shape[0],\\\n",
    "        '''feature and targets must have the number of rows\n",
    "        but in these case feature is {0} and target is {1}'''.format(x.shape[0],y.shape[0])\n",
    "        \n",
    "        n=x.shape[0]  #this is the amount of training data we have \n",
    "        for i in range(0,n,size):\n",
    "            x_batch,y_batch=x[i:i+size],y[i:i+size]\n",
    "            yield x_batch,y_batch\n",
    "\n",
    "    \n",
    "    def fit(self,x_train:np.ndarray,y_train:np.ndarray,\n",
    "           x_test:np.ndarray,y_test:np.ndarray,\n",
    "           epochs:int=100,eval_every:int=32,batch_sz:int=32,seed:int=32,restart:bool=True)->None:\n",
    "        ''' this trains the neural network '''\n",
    "        np.random.seed(seed)\n",
    "        if restart:\n",
    "            ''' this restart the weights or the parameters to some initial values'''\n",
    "            for layer in self.net.layers:\n",
    "                layer.first=True\n",
    "            self.best_loss=1e9\n",
    "            \n",
    "        for i in range(epochs):\n",
    "            #this loop goes on unil the training is Done\n",
    "            if (i+1)%eval_every == 0:\n",
    "                #this is sort of early stoping\n",
    "                last_model=deepcopy(self.net)\n",
    "                \n",
    "            x_train,y_train=permute_data(x_train,y_train)  #this shuffles the data\n",
    "            batch_generator=self.generate_batches(x_train,y_train,batch_sz)\n",
    "            for j,(x_batch,y_batch) in enumerate(batch_generator):\n",
    "                self.net.train_batch(x_batch,y_batch)\n",
    "                self.optim.step()\n",
    "                \n",
    "            if (i+1)%eval_every==0:\n",
    "                #this check for what it had learned every some terms on the test set\n",
    "                test_preds=self.net.forward(x_test)\n",
    "                loss=self.net.loss.forward(test_preds,y_test)\n",
    "                #this is basically Early stoping\n",
    "                if loss < self.best_loss:\n",
    "                    print(f\"validation(test) loss after {i+1} epochs is {loss:.3f}\")\n",
    "                    self.best_loss=loss\n",
    "                else:\n",
    "                    print(f\"loss increased after {i+1} epochs,final good loss is {self.best_loss:.3f}\")\n",
    "                    self.net=last_model\n",
    "                    setattr(self.optim,\"net\",self.net)\n",
    "                    break\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2eab6c-6536-4a10-8cce-047c352ced54",
   "metadata": {},
   "source": [
    "## error evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "96a33d65-e500-4d63-a668-fbaeeb04c337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(y_true:np.ndarray,y_pred:np.ndarray):\n",
    "    ''' performing the MAE for neurla network'''\n",
    "    return np.mean(np.abs(y_true-y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "9b777b7d-2b33-4a97-8241-d28c17bb2cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true:np.ndarray,y_pred:np.ndarray):\n",
    "    ''' this performs the RMSE'''\n",
    "    return np.sqrt(np.mean(np.power(y_true-y_pred,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "9180051d-b055-4073-9744-b5a922a9c373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_regression_model(model:neuralnetwork,\n",
    "                          x_test:np.ndarray,\n",
    "                          y_test:np.ndarray):\n",
    "    ''' this computes the mae and rmse '''\n",
    "    preds=model.forward(x_test)\n",
    "    preds=preds.reshape(-1,1)\n",
    "    print(\" the mean absolute error is: {:.2f}\".format(mae(y_test,preds)))\n",
    "    print() #empty line\n",
    "    print(\"root mean squred error:{:.2f}\".format(rmse(y_test,preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80642bc-8058-4e4a-8799-cd0e6f98e845",
   "metadata": {},
   "source": [
    "# lets train our deep neural network that we just created YAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "a7025e5a-a107-4800-8312-da324683c78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets test how our deep neural network\n",
    "lr=neuralnetwork(\n",
    "    layers=[dense(neurons=1,\n",
    "                 activation=linear())],\n",
    "    loss=meansquarederror(),\n",
    "    seed=42\n",
    ")\n",
    "nn=neuralnetwork(\n",
    "    layers=[\n",
    "        dense(neurons=13,\n",
    "           activation=sigmoid()),\n",
    "        dense(neurons=1,\n",
    "             activation=linear())\n",
    "    ],\n",
    "    loss=meansquarederror(),\n",
    "    seed=42\n",
    ")\n",
    "dl=neuralnetwork(\n",
    "    layers=[\n",
    "        dense(neurons=13,activation=sigmoid()),\n",
    "        dense(neurons=13,activation=sigmoid()),\n",
    "        dense(neurons=13,activation=linear())\n",
    "    ],\n",
    "    loss=meansquarederror(),\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc29ede-1227-4f90-80dc-b606a7b1332a",
   "metadata": {},
   "source": [
    "## lets some data and do some EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "0c9248ca-8958-4edd-ad4e-2c177e8ee08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "target = raw_df.values[1::2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "91a5440a-3209-4f60-917d-e49514519fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "s=StandardScaler()\n",
    "data=s.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "85ad637d-b9e9-4df2-bfb2-fa0e750b6a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(data,target,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "760a90ff-88ea-4ba3-b902-3a502e5721fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=y_train.reshape(-1,1)\n",
    "y_test=y_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f30026-5939-488e-918c-3c56decaf4df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7134474-f87e-4d89-8098-e78e014295e3",
   "metadata": {},
   "source": [
    "### lets train one of our 3 model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "3130c4b9-3596-421d-9c2f-e72d12f09ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation(test) loss after 10 epochs is 652.977\n",
      "loss increased after 20 epochs,final good loss is 652.977\n",
      "\n",
      " the mean absolute error is: 23.50\n",
      "\n",
      "root mean squred error:25.55\n"
     ]
    }
   ],
   "source": [
    "trainer=Trainer(lr,SGD(lr=0.01))\n",
    "trainer.fit(x_train,y_train,x_test,y_test,epochs=100,\n",
    "           eval_every=10,seed=42)\n",
    "print()\n",
    "eval_regression_model(lr,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "5e51b5b9-2050-48f1-9854-92dbdff0d853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation(test) loss after 10 epochs is 551.293\n",
      "loss increased after 20 epochs,final good loss is 551.293\n",
      "\n",
      " the mean absolute error is: 21.99\n",
      "\n",
      "root mean squred error:23.48\n"
     ]
    }
   ],
   "source": [
    "trainer=Trainer(nn,SGD(lr=0.01))\n",
    "trainer.fit(x_train,y_train,x_test,y_test,\n",
    "           eval_every=10,seed=42)\n",
    "print()\n",
    "eval_regression_model(nn,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f015d5cd-a785-4bb1-8dda-569300dfed63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84534461-4ef8-4151-9417-c362f10df1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE END "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f624d1-67c8-46cb-9a0b-5ccf24964d60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
